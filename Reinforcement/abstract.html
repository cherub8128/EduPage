<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>강화학습(RL) 핵심 개념 (상세 버전)</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" xintegrity="sha384-n8MVd4RsNIU07RLA8HuPOhrBIWEidUdaf65IBqz2tVRckQ9/VEOESTP1OF2dvGub" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" xintegrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUbKyIyuh" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" xintegrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Noto Sans KR', sans-serif;
            background-color: #f7fafc;
        }
        .katex-display { display: block; margin: 0.5em 0; text-align: center; }
        .katex { font-size: 1.1em; }
        .section-card {
            background-color: white;
            border-radius: 1rem;
            padding: 2rem;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
            margin-bottom: 2rem;
        }
        .gradient-text {
            background-image: linear-gradient(to right, #4c51bf, #b794f4);
            -webkit-background-clip: text;
            background-clip: text;
            color: transparent;
        }
        .eq-box {
            background-color: #f7fafc;
            padding: 1rem;
            border-radius: 0.5rem;
            border-left: 4px solid #667eea;
            margin-top: 1rem;
            overflow-x: auto;
        }
    </style>
</head>
<body class="p-4 md:p-8">
    <div class="max-w-7xl mx-auto">
        <!-- Header -->
        <header class="text-center mb-12">
            <h1 class="text-4xl md:text-6xl font-bold gradient-text">강화학습(RL) 핵심 완전 정복</h1>
            <p class="text-lg text-gray-600 mt-4">수학적 기초부터 최신 알고리즘까지, 강화학습의 모든 것을 파헤쳐봅니다.</p>
        </header>

        <!-- 1. MDP: 강화학습의 뼈대 -->
        <section class="section-card">
            <h2 class="text-3xl font-bold text-gray-800 mb-4">1. MDP: 강화학습의 수학적 뼈대</h2>
            <p class="text-gray-700 mb-6">
                강화학습은 <strong>마르코프 의사결정 과정(Markov Decision Process, MDP)</strong>이라는 수학적 틀 안에서 문제를 정의하고 해결책을 찾습니다. MDP는 에이전트와 환경의 상호작용을 모델링하며, 다음과 같은 요소로 구성됩니다.
            </p>
            <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6 text-center">
                <div class="bg-indigo-50 p-4 rounded-lg">
                    <p class="font-bold text-indigo-800 text-lg">상태 ($S$)</p>
                    <p class="text-sm text-indigo-700">에이전트가 관찰하는 현재 상황</p>
                </div>
                <div class="bg-indigo-50 p-4 rounded-lg">
                    <p class="font-bold text-indigo-800 text-lg">행동 ($A$)</p>
                    <p class="text-sm text-indigo-700">상태 $S$에서 할 수 있는 선택지</p>
                </div>
                <div class="bg-indigo-50 p-4 rounded-lg">
                    <p class="font-bold text-indigo-800 text-lg">보상 ($R$)</p>
                    <p class="text-sm text-indigo-700">행동에 대한 환경의 즉각적인 피드백</p>
                </div>
                <div class="bg-indigo-50 p-4 rounded-lg">
                    <p class="font-bold text-indigo-800 text-lg">정책 ($\pi$)</p>
                    <p class="text-sm text-indigo-700">상태에 따른 행동 지침 (전략)</p>
                </div>
            </div>
        </section>

        <!-- 2. 가치 함수와 벨만 방정식 -->
        <section class="section-card">
            <h2 class="text-3xl font-bold text-gray-800 mb-4">2. 가치 함수와 벨만 방정식: 최적의 길 찾기</h2>
            <p class="text-gray-700 mb-6">
                "현재 상태가 미래에 얼마나 좋은가?"를 알려주는 <strong>가치 함수</strong>는 최적의 정책을 찾는 핵심 열쇠입니다. <strong>벨만 방정식</strong>은 현재 상태의 가치와 다음 상태의 가치 사이의 관계를 정의하여, 이 문제를 풀 수 있는 실마리를 제공합니다.
            </p>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                <div>
                    <h3 class="text-xl font-semibold mb-2">가치 함수 (Value Functions)</h3>
                    <p><strong>상태 가치 함수 $V^\pi(s)$</strong>: 현재 상태 $s$에서 시작하여 정책 $\pi$를 따랐을 때 기대되는 미래 보상의 총합.</p>
                    <div class="eq-box">$V^\pi(s) = \mathbb{E}_\pi [G_t | S_t = s] \quad \text{where } G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$</div>
                    <p class="mt-4"><strong>행동 가치 함수 $Q^\pi(s,a)$</strong>: 상태 $s$에서 행동 $a$를 한 후, 정책 $\pi$를 따랐을 때의 기대 보상 총합.</p>
                    <div class="eq-box">$Q^\pi(s,a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a]$</div>
                </div>
                <div>
                    <h3 class="text-xl font-semibold mb-2">벨만 최적 방정식 (Bellman Optimality Eq.)</h3>
                    <p>모든 정책 중에서 가장 높은 가치를 주는 최적 가치 함수 $V^*$와 $Q^*$는 이 방정식을 만족합니다. 이 방정식을 푸는 것이 RL의 목표입니다.</p>
                    <div class="eq-box">$Q^*(s,a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} Q^*(S_{t+1}, a') | S_t=s, A_t=a]$</div>
                </div>
            </div>
        </section>

        <!-- 3. 모델-프리 학습: 경험으로 배우기 -->
        <section class="section-card">
            <h2 class="text-3xl font-bold text-gray-800 mb-4">3. 모델-프리(Model-Free) 학습: 경험으로 배우기</h2>
            <p class="text-gray-700 mb-6">
                환경의 모든 규칙(모델)을 알지 못할 때, 에이전트는 직접 부딪히며 얻은 경험(샘플)을 통해 학습해야 합니다.
            </p>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                <div class="bg-gray-50 p-6 rounded-lg">
                    <h3 class="text-xl font-semibold mb-2">몬테카를로 (MC) 학습</h3>
                    <p>하나의 에피소드가 끝날 때까지 기다린 후, 실제로 얻은 보상의 총합($G_t$)으로 가치를 업데이트합니다. 분산이 크지만 편향이 없는 특징이 있습니다.</p>
                    <div class="eq-box">$V(S_t) \leftarrow V(S_t) + \alpha(G_t - V(S_t))$</div>
                </div>
                <div class="bg-gray-50 p-6 rounded-lg">
                    <h3 class="text-xl font-semibold mb-2">시간차 (TD) 학습</h3>
                    <p>한 스텝마다 다음 상태의 추정 가치를 이용해 현재 가치를 업데이트(부트스트래핑)합니다. MC보다 효율적이며, <strong>SARSA</strong>와 <strong>Q-Learning</strong>이 대표적입니다.</p>
                    <div class="eq-box">$Q(S,A) \leftarrow Q(S,A) + \alpha[R + \gamma Q(S',A') - Q(S,A)]$</div>
                </div>
            </div>
        </section>
        
        <!-- 4. 딥 강화학습: 심층 신경망과의 만남 -->
        <section class="section-card">
            <h2 class="text-3xl font-bold text-gray-800 mb-4">4. 딥 강화학습(DRL): 신경망과의 만남</h2>
            <p class="text-gray-700 mb-6">
                바둑판의 모든 경우의 수나 게임 화면의 모든 픽셀처럼 상태 공간이 거대할 때, <strong>심층 신경망(DNN)</strong>을 이용해 가치 함수나 정책을 근사합니다. <strong>DQN</strong>은 DRL의 혁신적인 시작을 알렸습니다.
            </p>
            <h3 class="text-xl font-semibold mb-2">DQN (Deep Q-Network)의 핵심 혁신</h3>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8 mt-4">
                <div class="bg-blue-50 p-6 rounded-lg">
                    <h4 class="font-bold text-lg text-blue-800">경험 리플레이 (Experience Replay)</h4>
                    <p class="text-blue-700">경험 $(s, a, r, s')$을 버퍼에 저장하고 무작위로 샘플링하여 학습합니다. 데이터 간의 상관관계를 깨고 학습 안정성을 높입니다.</p>
                </div>
                <div class="bg-blue-50 p-6 rounded-lg">
                    <h4 class="font-bold text-lg text-blue-800">타겟 네트워크 (Target Networks)</h4>
                    <p class="text-blue-700">업데이트 목표(TD Target)를 계산하는 별도의 네트워크를 두어, 목표값이 흔들리지 않게 고정합니다. 이를 통해 학습을 안정화합니다.</p>
                </div>
            </div>
            <div class="eq-box">
                $\mathcal{L}(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} [(r + \gamma \max_{a'} Q(s',a'; \theta_{target}) - Q(s,a; \theta))^2]$
            </div>
        </section>
        
        <!-- 5. 정책 경사와 액터-크리틱 -->
        <section class="section-card">
            <h2 class="text-3xl font-bold text-gray-800 mb-4">5. 정책 경사와 액터-크리틱: 직접 정책을 찾아서</h2>
             <p class="text-gray-700 mb-6">
                가치 함수를 거치지 않고, 보상을 최대로 하는 정책 $\pi(a|s; \theta)$의 파라미터 $\theta$를 직접 최적화하는 방법론입니다.
            </p>
            <div class="grid grid-cols-1 lg:grid-cols-2 gap-8">
                <div>
                    <h3 class="text-xl font-semibold mb-2">정책 경사 (Policy Gradient, PG)</h3>
                    <p>정책이 좋은 행동을 할 확률은 높이고(log-probability 증가), 나쁜 행동을 할 확률은 낮추는 방향으로 업데이트합니다. <strong>REINFORCE</strong>가 대표적입니다.</p>
                    <div class="eq-box">$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [A(S_t, A_t) \nabla_\theta \log \pi_\theta(A_t|S_t)]$</div>
                </div>
                <div>
                    <h3 class="text-xl font-semibold mb-2">액터-크리틱 (Actor-Critic, AC)</h3>
                    <p>정책을 학습하는 <strong>액터(Actor)</strong>와, 그 정책의 가치를 평가하는 <strong>크리틱(Critic)</strong>으로 역할을 분담하여 안정적이고 효율적으로 학습합니다.</p>
                    <ul class="list-disc list-inside mt-2 text-gray-600">
                        <li><strong>PPO</strong>: 정책이 급격히 변하지 않도록 제한하여 안정성 확보</li>
                        <li><strong>SAC</strong>: 엔트로피를 최대화하여 탐색을 장려하고 성능 향상</li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- 6. 최신 연구 동향 -->
        <section class="section-card">
            <h2 class="text-3xl font-bold text-gray-800 mb-4">6. 최신 연구 동향</h2>
            <div class="grid grid-cols-1 md:grid-cols-3 gap-6">
                <div class="bg-green-50 p-6 rounded-lg">
                    <h4 class="font-bold text-lg text-green-800">모델 기반 RL</h4>
                    <p class="text-sm text-green-700">환경의 모델을 학습하여 가상 세계에서 계획(planning)을 수행합니다. 데이터 효율성이 높으며 <strong>AlphaZero, MuZero</strong>가 대표적입니다.</p>
                </div>
                <div class="bg-green-50 p-6 rounded-lg">
                    <h4 class="font-bold text-lg text-green-800">모방 학습 (IL)</h4>
                    <p class="text-sm text-green-700">전문가의 행동 데이터를 모방하여 정책을 학습합니다. 보상 설계가 어려운 문제에 효과적이며 <strong>GAIL</strong> 등이 있습니다.</p>
                </div>
                <div class="bg-green-50 p-6 rounded-lg">
                    <h4 class="font-bold text-lg text-green-800">인간 피드백 기반 RL (RLHF)</h4>
                    <p class="text-sm text-green-700">인간의 선호도를 보상 모델로 학습하여 정책을 미세 조정합니다. <strong>ChatGPT</strong>와 같은 대규모 언어 모델의 핵심 기술입니다.</p>
                </div>
            </div>
        </section>

    </div>

    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ],
                throwOnError : false
            });
        });
    </script>
</body>
</html>
