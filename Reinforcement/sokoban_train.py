import os
import torch
import torch.nn as nn
from gymnasium import spaces

from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.callbacks import CheckpointCallback

from rnd_wrapper import RNDRewardWrapper 
from sokoban_env import SokobanEnv

class CustomCNN(BaseFeaturesExtractor):
    def __init__(self, observation_space: spaces.Box, features_dim: int = 128):
        super().__init__(observation_space, features_dim)
        n_input_channels = observation_space.shape[0]
        self.cnn = nn.Sequential(
            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Flatten(),
        )
        with torch.no_grad():
            n_flatten = self.cnn(
                torch.as_tensor(observation_space.sample()[None]).float()
            ).shape[1]
        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        return self.linear(self.cnn(observations))


# í™˜ê²½ ìƒì„±ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
def make_env(log_dir):
    def _init():
        env = SokobanEnv()
        # RND ë˜í¼ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
        env = RNDRewardWrapper(env, lr=1e-4, feature_dim=128)
        env = Monitor(env, log_dir)
        return env
    return _init

if __name__ == '__main__':
    log_dir = "sokoban_logs_rnd_manual/"
    tensorboard_log_dir = "sokoban_tensorboard_rnd_manual/"
    model_save_path = "sokoban_models_rnd_manual/"

    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(tensorboard_log_dir, exist_ok=True)
    os.makedirs(model_save_path, exist_ok=True)

    env = DummyVecEnv([make_env(log_dir)])

    policy_kwargs = dict(
        features_extractor_class=CustomCNN,
        features_extractor_kwargs=dict(features_dim=128),
    )

    # RND ë˜í¼ê°€ ë³´ìƒì„ ì²˜ë¦¬í•´ì£¼ë¯€ë¡œ, ì¼ë°˜ PPO ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    model = PPO(
        "CnnPolicy",
        env,
        policy_kwargs=policy_kwargs,
        verbose=1,
        tensorboard_log=tensorboard_log_dir,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=128, # RND ì‚¬ìš© ì‹œ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë¦¬ë©´ ì•ˆì •ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        n_epochs=10,
        gamma=0.99,
        ent_coef=0.01,
    )
    
    checkpoint_callback = CheckpointCallback(
        save_freq=50000,
        save_path=model_save_path,
        name_prefix="sokoban_rnd_manual_model"
    )

    print("--- ì§ì ‘ êµ¬í˜„í•œ RNDë¥¼ ì‚¬ìš©í•œ í•™ìŠµ ì‹œì‘ ---")
    model.learn(
        total_timesteps=1_000_000,
        callback=checkpoint_callback,
        progress_bar=True
    )

    final_model_path = os.path.join(model_save_path, "sokoban_rnd_manual_final")
    model.save(final_model_path)
    
    env.close()

    print("\n" + "="*50)
    print("ğŸ‰ í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ìµœì¢… ëª¨ë¸ì€ '{final_model_path}.zip'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("í•™ìŠµ ê³¼ì •ì„ í™•ì¸í•˜ë ¤ë©´ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ í„°ë¯¸ë„ì— ì…ë ¥í•˜ì„¸ìš”:")
    print(f"tensorboard --logdir={tensorboard_log_dir}")
    print("="*50)