<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>강화학습(RL) 발전사 타임라인 (GRPO 추가)</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Noto Sans KR', sans-serif;
            background-color: #f3f4f6;
        }
        .gradient-text {
            background-image: linear-gradient(to right, #3b82f6, #8b5cf6);
            -webkit-background-clip: text;
            background-clip: text;
            color: transparent;
        }
        .timeline {
            position: relative;
            max-width: 1200px;
            margin: 0 auto;
        }
        .timeline::after {
            content: '';
            position: absolute;
            width: 6px;
            background-color: #d1d5db;
            top: 0;
            bottom: 0;
            left: 50%;
            margin-left: -3px;
            border-radius: 3px;
        }
        .timeline-container {
            padding: 10px 40px;
            position: relative;
            background-color: inherit;
            width: 50%;
        }
        .timeline-container::after {
            content: '';
            position: absolute;
            width: 25px;
            height: 25px;
            right: -12.5px;
            background-color: white;
            border: 4px solid #3b82f6;
            top: 25px;
            border-radius: 50%;
            z-index: 1;
        }
        .left { left: 0; }
        .right { left: 50%; }
        .left::before {
            content: " "; height: 0; position: absolute; top: 32px; width: 0; z-index: 1; right: 30px; border: medium solid white; border-width: 10px 0 10px 10px; border-color: transparent transparent transparent white;
        }
        .right::before {
            content: " "; height: 0; position: absolute; top: 32px; width: 0; z-index: 1; left: 30px; border: medium solid white; border-width: 10px 10px 10px 0; border-color: transparent white transparent transparent;
        }
        .right::after { left: -12.5px; }
        .content {
            padding: 20px 30px; background-color: white; position: relative; border-radius: 0.5rem; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06); border-left: 5px solid #3b82f6;
        }
        .right .content {
             border-left: none; border-right: 5px solid #8b5cf6;
        }
        .year-tag {
            font-size: 1.25rem; font-weight: bold; color: #4f46e5;
        }
        @media screen and (max-width: 768px) {
            .timeline::after { left: 31px; }
            .timeline-container { width: 100%; padding-left: 70px; padding-right: 25px; }
            .timeline-container::before { left: 60px; border: medium solid white; border-width: 10px 10px 10px 0; border-color: transparent white transparent transparent; }
            .left::after, .right::after { left: 18px; }
            .right { left: 0%; }
        }
    </style>
</head>
<body class="p-4 md:p-8">
    <div class="max-w-7xl mx-auto">
        <header class="text-center mb-16">
            <h1 class="text-4xl md:text-6xl font-bold gradient-text">강화학습(RL) 발전의 여정</h1>
            <p class="text-lg text-gray-600 mt-4">주요 알고리즘의 탄생과 진화 타임라인</p>
        </header>

        <div class="timeline">
            <div class="timeline-container left"><div class="content"><p class="year-tag">1989 - 1996</p><h2 class="text-xl font-bold text-gray-800">모델-프리 학습의 여명</h2><p class="text-gray-700"><strong>Q-Learning (1989)</strong>, <strong>REINFORCE (1992)</strong>, <strong>SARSA (1996)</strong> 등 현대 RL의 근간이 되는 알고리즘들이 등장했습니다.</p></div></div>
            <div class="timeline-container right"><div class="content"><p class="year-tag">2013</p><h2 class="text-xl font-bold text-gray-800">DQN: 딥러닝과의 만남</h2><p class="text-gray-700">Q-러닝에 심층 신경망을 결합하여 <strong>딥 강화학습(DRL)</strong> 시대를 열었습니다. 경험 리플레이와 타겟 네트워크로 학습 안정성을 확보했습니다.</p></div></div>
            <div class="timeline-container left"><div class="content"><p class="year-tag">2015 - 2016</p><h2 class="text-xl font-bold text-gray-800">DRL의 폭발적 성장</h2><p class="text-gray-700"><strong>DQN 개선 알고리즘들</strong>, 연속 행동 공간을 위한 <strong>DDPG</strong>, 병렬 학습 방식의 <strong>A3C</strong>, 그리고 세상을 놀라게 한 <strong>AlphaGo</strong>가 등장했습니다.</p></div></div>
            <div class="timeline-container right"><div class="content"><p class="year-tag">2017</p><h2 class="text-xl font-bold text-gray-800">안정성의 시대</h2><p class="text-gray-700"><strong>PPO</strong>가 정책 업데이트를 안정화시키며 현재까지 널리 쓰이는 표준 알고리즘으로 자리 잡았고, <strong>AlphaZero</strong>는 더 범용적인 성능을 보여주었습니다.</p></div></div>
            <div class="timeline-container left"><div class="content"><p class="year-tag">2018 - 2019</p><h2 class="text-xl font-bold text-gray-800">성능과 모델 기반의 도약</h2><p class="text-gray-700"><strong>TD3</strong>와 <strong>SAC</strong>가 연속 제어에서 최고의 성능을 갱신했고, <strong>MuZero</strong>는 규칙 없이 스스로 모델을 학습하며 모델 기반 RL의 정점을 찍었습니다.</p></div></div>
            <div class="timeline-container right"><div class="content"><p class="year-tag">2020 ~</p><h2 class="text-xl font-bold text-gray-800">인간과의 상호작용</h2><p class="text-gray-700"><strong>RLHF</strong>가 인간의 피드백을 통해 언어 모델을 정렬하는 핵심 기술로 부상하며 ChatGPT와 같은 AI의 패러다임을 바꾸었습니다.</p></div></div>
            <div class="timeline-container left"><div class="content"><p class="year-tag">2024 ~</p><h2 class="text-xl font-bold text-gray-800">GRPO: 효율성의 혁신</h2><p class="text-gray-700">PPO를 극도로 최적화하여 가치 및 보상 모델을 제거. RLVR(검증 가능한 보상)을 활용해 메모리 사용량과 속도를 혁신적으로 개선하며, 대규모 추론 모델 훈련의 새로운 방향을 제시합니다.</p></div></div>
        </div>
    </div>
</body>
</html>
